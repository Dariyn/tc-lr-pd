---
phase: 01-data-pipeline-foundation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified: [src/pipeline/data_loader.py, src/pipeline/schema.py, requirements.txt]
autonomous: true
---

<objective>
Establish data loading infrastructure that reads Excel/CSV work order files and validates schema integrity.

Purpose: Create the foundation for all downstream analysis by ensuring data can be reliably loaded and conforms to expected structure.
Output: Data loader module with schema validation, initial requirements.txt, and verified ability to load the sample dataset.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

**Data file location:** `input/adhoc_wo_20240101_20250531.xlsx - in.csv`

**Known schema fields (from data inspection):**
- Core IDs: id_, wo_no, work_request_no
- Equipment: Equipment_ID, EquipmentNumber, EquipmentName
- Categorization: Property_category, FM_Type, Work_Order_Type, service_type_lv1/lv2/lv3
- Dates: Create_Date, Complete_Date, Close_Date, Request_Create_Date, etc.
- Cost: PO_AMOUNT
- Location: Cluster, Property, Property_Code, location_desc
- Vendor: Contractor, contract_id
- Problem tracking: Problem, Cause, Remedy, description

**Constraints from PROJECT.md:**
- Working with historical exports only
- Batch processing (not real-time)
- Focus on accuracy for cost reduction insights
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create project structure and dependencies</name>
  <files>requirements.txt, src/pipeline/__init__.py, src/__init__.py</files>
  <action>
Create Python project structure:
- src/pipeline/ directory for data pipeline modules
- requirements.txt with pandas (Excel/CSV reading), openpyxl (Excel support), python-dateutil (date parsing)
- Use pandas for data loading (not csv module) - better handling of mixed types, dates, and large files
- Add __init__.py files for proper package structure
  </action>
  <verify>ls src/pipeline/ shows __init__.py, cat requirements.txt shows pandas and openpyxl</verify>
  <done>Project structure exists with required dependencies defined</done>
</task>

<task type="auto">
  <name>Task 2: Implement schema definition and validation</name>
  <files>src/pipeline/schema.py</files>
  <action>
Create schema.py defining expected work order fields:
- Define REQUIRED_FIELDS list: ['id_', 'wo_no', 'Equipment_ID', 'EquipmentName', 'Create_Date', 'Complete_Date', 'PO_AMOUNT', 'Property_category', 'FM_Type', 'Work_Order_Type']
- Define OPTIONAL_FIELDS for additional fields we'll use: service_type levels, dates, location, contractor
- Define FIELD_TYPES dict mapping field names to expected pandas dtypes (use 'object' for most, let data_loader handle conversion)
- Create validate_schema(df) function that checks required fields exist and returns list of missing fields
- Include docstrings explaining each field's purpose for analysis
  </action>
  <verify>python -c "from src.pipeline.schema import REQUIRED_FIELDS, validate_schema; print(len(REQUIRED_FIELDS))" outputs count</verify>
  <done>Schema module defines required fields and provides validation function</done>
</task>

<task type="auto">
  <name>Task 3: Implement data loader with validation</name>
  <files>src/pipeline/data_loader.py</files>
  <action>
Create data_loader.py with load_work_orders(file_path) function:
- Use pandas.read_csv() with encoding='utf-8-sig' (handles BOM), low_memory=False (mixed types)
- Call validate_schema() to check required fields - raise ValueError with helpful message if missing fields
- Convert date columns to datetime using pd.to_datetime() with errors='coerce' (invalid dates become NaT)
- Convert PO_AMOUNT to numeric with pd.to_numeric() with errors='coerce' (non-numeric becomes NaN)
- Strip whitespace from string columns using df.apply(lambda x: x.str.strip() if x.dtype == "object" else x)
- Log row count and date range (min/max Create_Date) for verification
- Return pandas DataFrame
- Include comprehensive error handling with informative messages for file not found, encoding issues, etc.
  </action>
  <verify>python -c "from src.pipeline.data_loader import load_work_orders; df = load_work_orders('input/adhoc_wo_20240101_20250531.xlsx - in.csv'); print(f'{len(df)} rows loaded')" shows row count</verify>
  <done>Data loader successfully reads CSV file, validates schema, performs basic type conversions, and returns DataFrame</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] requirements.txt contains pandas, openpyxl
- [ ] src/pipeline/schema.py defines required fields and validation
- [ ] src/pipeline/data_loader.py loads sample data without errors
- [ ] Verification command confirms data loads correctly with row count
</verification>

<success_criteria>

- All tasks completed
- Sample data file loads successfully
- Schema validation catches missing required fields
- Date and numeric conversions handled gracefully
- No errors or warnings during data load
  </success_criteria>

<output>
After completion, create `.planning/phases/01-data-pipeline-foundation/01-01-SUMMARY.md`
</output>
