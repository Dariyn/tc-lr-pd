---
phase: 06-integration-testing
plan: 02
type: execute
wave: 2
depends_on: ["06-01"]
files_modified: [tests/test_end_to_end.py, tests/fixtures/sample_work_orders.csv]
autonomous: true
---

<objective>
Create end-to-end integration tests that validate complete pipeline with realistic sample data.

Purpose: Ensure full pipeline works correctly from raw work order data through all analysis steps to final outputs (reports, exports, visualizations). Provides regression protection and validates that all 16 modules integrate correctly.

Output: Comprehensive end-to-end test suite with sample data fixtures that exercise the complete workflow and verify all output artifacts.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

# Dependencies - we're testing the orchestrator
@.planning/phases/06-integration-testing/06-01-SUMMARY.md

# Understanding expected outputs from all phases
@.planning/phases/04-reporting-engine/04-02-SUMMARY.md
@.planning/phases/04-reporting-engine/04-03-SUMMARY.md
@.planning/phases/05-data-export-visualization/05-01-SUMMARY.md
@.planning/phases/05-data-export-visualization/05-02-SUMMARY.md
@.planning/phases/05-data-export-visualization/05-03-SUMMARY.md

# Source code
@src/orchestrator/pipeline_orchestrator.py
@main.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create realistic sample data fixture</name>
  <files>tests/fixtures/sample_work_orders.csv</files>
  <action>
Create tests/fixtures/ directory and sample_work_orders.csv with realistic work order data (50-100 rows) that exercises all analysis code paths:

Required columns (matching schema from Phase 1):
- Work_order_id: Unique identifiers (WO-001, WO-002, ...)
- Equipment_ID: Multiple equipment (EQ-001 to EQ-020, some with multiple work orders)
- Equipment_Name: Descriptive names (Air Conditioner, Elevator, Water Pump, etc.)
- Category: Mix of FM_Type and service_type_lv2 values
- service_type_lv2: Categories like HVAC, Electrical, Plumbing
- FM_Type: Broader categories like Mechanical, Electrical
- Contractor: Vendor names (Vendor A, Vendor B, Unknown, blank for variety)
- PO_AMOUNT: Mix of costs (0 for adhoc, various amounts from $100 to $5000)
- Work_order_open_date: Dates spread across 12 months (YYYY-MM-DD format)
- Work_order_close_date: Close dates 1-30 days after open dates
- Work_Order_Description: Text with failure patterns (leak, broken, malfunction, electrical issue, etc.)

Data characteristics to test all code paths:
- Some equipment with high work order frequency (5-10 WOs) to trigger outlier detection
- Seasonal variation (more HVAC work in summer/winter)
- Multiple vendors with varying costs and quality
- Recurring failure patterns in descriptions (pipe leak, AC malfunction, elevator error)
- Missing values (some blank contractors, some missing categories)
- Edge cases (single work order equipment, same-day completion, zero cost)

Generate programmatically in test or create manually - aim for 50-100 rows that feel realistic and exercise all analysis logic.
  </action>
  <verify>python -c "import pandas as pd; df = pd.read_csv('tests/fixtures/sample_work_orders.csv'); print(f'{len(df)} rows, {len(df.columns)} columns'); print(df.head())"</verify>
  <done>Sample CSV created with 50-100 realistic work orders, includes all required columns, exercises all code paths</done>
</task>

<task type="auto">
  <name>Task 2: Create end-to-end integration tests</name>
  <files>tests/test_end_to_end.py</files>
  <action>
Create comprehensive end-to-end test suite that validates complete pipeline:

Test structure (10 tests minimum):

Pipeline execution tests (3 tests):
- test_e2e_full_pipeline: Run complete analysis from sample data, verify no errors
- test_e2e_analysis_results: Verify all analysis results produced (equipment, seasonal, vendor, failure)
- test_e2e_execution_time: Verify pipeline completes in reasonable time (<30 seconds)

Report generation tests (2 tests):
- test_e2e_pdf_report: Verify PDF report created, file size > 50KB, valid PDF
- test_e2e_excel_report: Verify Excel report created, file size > 20KB, valid Excel with 6 sheets

Data export tests (2 tests):
- test_e2e_csv_exports: Verify 4 CSV files created (equipment, seasonal, vendor, failure)
- test_e2e_json_exports: Verify 4 JSON files created, valid JSON, parseable

Visualization tests (2 tests):
- test_e2e_static_charts: Verify 4 PNG charts created, valid image format
- test_e2e_interactive_dashboard: Verify HTML dashboard created, contains plotly.js, valid HTML

CLI integration test (1 test):
- test_e2e_cli_interface: Run main.py analyze via subprocess, verify exit code 0, verify outputs created

Implementation notes:
- Use sample_work_orders.csv fixture
- Use tmp_path for outputs to avoid polluting workspace
- Verify file existence AND validity (parse PDF/Excel/JSON/HTML to ensure they're not corrupt)
- For PDF: check magic bytes b'%PDF'
- For Excel: use openpyxl to verify sheets exist
- For JSON: use json.load() to verify parseable
- For HTML: check for <!DOCTYPE html> and <script> tags
- Use subprocess.run() for CLI test with capture_output=True
- All tests should be independent and use fresh output directories

Test data validation:
- Verify sample data produces expected patterns:
  * At least 3 consensus outlier equipment
  * Seasonal variation detected
  * Multiple vendors analyzed
  * Failure patterns identified
  </action>
  <verify>pytest tests/test_end_to_end.py -v -s --tb=short</verify>
  <done>10+ end-to-end tests covering complete pipeline execution, all output formats, CLI interface. All tests pass, validating full integration.</done>
</task>

<task type="auto">
  <name>Task 3: Add smoke tests and performance benchmarks</name>
  <files>tests/test_end_to_end.py</files>
  <action>
Add additional tests to test_end_to_end.py for robustness and performance:

Smoke tests (4 tests):
- test_e2e_empty_input: Verify graceful handling of empty CSV (headers only)
- test_e2e_minimal_data: Verify pipeline works with minimal data (5 rows)
- test_e2e_missing_columns: Verify error handling for missing required columns
- test_e2e_corrupted_data: Verify handling of malformed CSV (bad dates, non-numeric costs)

Performance benchmarks (2 tests):
- test_e2e_performance_100_rows: Benchmark with 100 rows, record time
- test_e2e_performance_1000_rows: Benchmark with 1000 rows (generated programmatically), verify completes in <60 seconds

Data quality verification (2 tests):
- test_e2e_data_quality_report: Verify quality report generated and contains expected metrics
- test_e2e_quality_gate_pass: Verify pipeline continues when quality score > 85
- test_e2e_quality_gate_fail: Verify pipeline exits with code 1 when quality score < 85

Output content verification (3 tests):
- test_e2e_report_content: Open PDF/Excel and verify key sections present (equipment, seasonal, vendor, failure)
- test_e2e_export_accuracy: Load exported CSV/JSON and verify data matches analysis results
- test_e2e_dashboard_interactivity: Verify dashboard HTML contains all 4 charts and interactive elements

Implementation:
- Use parametrize for performance tests with different row counts
- Generate larger datasets programmatically (duplicate and modify sample data)
- Use markers (@pytest.mark.slow) for tests that take >5 seconds
- Add performance timing with time.time() and log results
- For content verification, sample key values rather than full validation (that's covered by unit tests)

Total test count after this task: 10 (task 2) + 11 (task 3) = 21+ end-to-end tests
  </action>
  <verify>pytest tests/test_end_to_end.py -v --tb=short -m "not slow" && pytest tests/test_end_to_end.py -v --tb=short -m slow</verify>
  <done>21+ end-to-end tests including smoke tests, performance benchmarks, quality gates, output validation. All tests pass, pipeline robust to edge cases.</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] pytest tests/test_end_to_end.py passes all tests (21+ tests)
- [ ] Sample data fixture created with realistic work orders
- [ ] End-to-end tests validate complete workflow from input to outputs
- [ ] Smoke tests verify error handling and edge cases
- [ ] Performance benchmarks confirm pipeline efficiency
- [ ] All output formats validated (PDF, Excel, CSV, JSON, PNG, HTML)
</verification>

<success_criteria>

- All tasks completed
- Realistic sample data fixture with 50-100 work orders
- Comprehensive end-to-end test suite with 21+ tests passing
- Pipeline validated from raw data through all outputs
- Smoke tests ensure robustness
- Performance benchmarks confirm efficiency (<30 sec for 100 rows)
- No errors or warnings introduced
  </success_criteria>

<output>
After completion, create `.planning/phases/06-integration-testing/06-02-SUMMARY.md`
</output>
